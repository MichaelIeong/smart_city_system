package edu.fudan.se.sctap_lowcode_tool.utils;

import com.fasterxml.jackson.databind.JsonNode;
import com.fasterxml.jackson.databind.ObjectMapper;
import com.fasterxml.jackson.databind.node.ObjectNode;
import org.apache.kafka.clients.consumer.ConsumerRecord;
import org.apache.kafka.clients.consumer.ConsumerRecords;
import org.apache.kafka.clients.consumer.KafkaConsumer;
import org.apache.kafka.common.errors.WakeupException;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.context.event.ContextClosedEvent;
import org.springframework.context.event.EventListener;
import org.springframework.stereotype.Component;
import org.springframework.boot.context.event.ApplicationReadyEvent;

import java.time.Duration;
import java.util.Collections;
import java.util.Properties;
import java.util.concurrent.BlockingQueue;
import java.util.concurrent.LinkedBlockingQueue;
import java.util.concurrent.atomic.AtomicBoolean;

@Component
public class KafkaConsumerUtil implements Runnable {

    private final KafkaConsumer<String, String> consumer;
    private final AtomicBoolean running = new AtomicBoolean(false);
    private final String topic;
    private Thread consumerThread;
    private final BlockingQueue<String> messageQueue = new LinkedBlockingQueue<>();
    private final ObjectMapper objectMapper = new ObjectMapper();

    public KafkaConsumerUtil(@Value("${kafka.bootstrap.servers}") String bootstrapServers,
                             @Value("${kafka.topic}") String topic) {
        this.topic = topic;

        Properties props = new Properties();
        props.put("bootstrap.servers", bootstrapServers);
        props.put("group.id", "sc_uos_consumer_group");
        props.put("enable.auto.commit", "true");
        props.put("auto.commit.interval.ms", "1000");
        props.put("key.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");
        props.put("value.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");
        props.put("auto.offset.reset", "latest");

        // é‡è¿ç›¸å…³é…ç½®
        props.put("reconnect.backoff.ms", "5000");         // ç¬¬ä¸€æ¬¡é‡è¿é—´éš”ï¼š5ç§’
        props.put("reconnect.backoff.max.ms", "60000");    // æœ€å¤§é‡è¿é—´éš”ï¼š60ç§’
        props.put("retry.backoff.ms", "1000");             // è¯·æ±‚å¤±è´¥åçš„é‡è¯•ç­‰å¾…æ—¶é—´ï¼š1ç§’

        this.consumer = new KafkaConsumer<>(props);
        this.consumer.subscribe(Collections.singletonList(topic));
    }

    @EventListener(ApplicationReadyEvent.class)
    public void startConsumerAfterAppReady() {
        new Thread(() -> {
            try {
                System.out.println("âœ… KafkaConsumerUtil æ­£åœ¨å¯åŠ¨ consumer...");
                running.set(true);
                run(); // å¯åŠ¨æ¶ˆè´¹çº¿ç¨‹
            } catch (Exception e) {
                System.err.println("âŒ KafkaConsumerUtil å¯åŠ¨å¤±è´¥ï¼š" + e.getMessage());
            }
        }, "kafka-consumer-thread").start();
    }

    @Override
    public synchronized void run() {
        try {
            while (running.get()) {
                ConsumerRecords<String, String> records = consumer.poll(Duration.ofMillis(1000));
                if (records.isEmpty()) continue;

                for (ConsumerRecord<String, String> record : records) {
                    long kafkaTimestamp = record.timestamp();
                    JsonNode originalJson;
                    try {
                        originalJson = objectMapper.readTree(record.value());
                    } catch (Exception e) {
                        ObjectNode fallbackJson = objectMapper.createObjectNode();
                        fallbackJson.put("value", record.value());
                        originalJson = fallbackJson;
                    }

                    if (originalJson.isObject()) {
                        ((ObjectNode) originalJson).put("timestamp", kafkaTimestamp);
                    } else {
                        ObjectNode wrapper = objectMapper.createObjectNode();
                        wrapper.set("data", originalJson);
                        wrapper.put("timestamp", kafkaTimestamp);
                        originalJson = wrapper;
                    }

                    // System.out.println("ğŸ“© æ¶ˆè´¹åˆ° Kafka æ¶ˆæ¯ï¼š" + originalJson.toPrettyString());
                    messageQueue.offer(originalJson.toString());
                }
            }
        } catch (WakeupException e) {
            if (running.get()) throw e;
        } finally {
            consumer.close();
        }
    }

    @EventListener(ContextClosedEvent.class)
    public void stopConsumer() {
        close();
        try {
            if (consumerThread != null) {
                consumerThread.join();
            }
        } catch (InterruptedException ignored) {}
    }

    public synchronized void close() {
        running.set(false);
        consumer.wakeup();
    }

    public String getLatestMessageBySensorId(int sensorId, long timeoutMillis) {
        long start = System.currentTimeMillis();

        while (System.currentTimeMillis() - start < timeoutMillis) {
            String message = messageQueue.poll();
            if (message != null) {
                try {
                    JsonNode json = objectMapper.readTree(message);
                    JsonNode idNode = json.get("id");
                    if (idNode != null && idNode.asInt() == sensorId) {
                        return message;
                    }
                } catch (Exception e) {
                    System.out.println("KafkaConsumerUtil: æ¶ˆæ¯è§£æå¤±è´¥ï¼š" + e.getMessage());
                }
            } else {
                try {
                    Thread.sleep(100);
                } catch (InterruptedException ignored) {}
            }
        }

        System.out.println("KafkaConsumerUtil: è¶…æ—¶æœªè·å–åˆ° sensorId=" + sensorId + " çš„æ¶ˆæ¯");
        return null;
    }
}